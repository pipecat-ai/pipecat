<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Pipecat Speech2Speech Test Client</title>
    <script src="https://cdn.jsdelivr.net/npm/protobufjs@7.4.0/dist/protobuf.min.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 50px auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #333;
            text-align: center;
        }
        .status {
            padding: 15px;
            margin: 20px 0;
            border-radius: 5px;
            font-weight: bold;
            text-align: center;
        }
        .status.disconnected {
            background-color: #fee;
            color: #c33;
        }
        .status.connected {
            background-color: #efe;
            color: #3c3;
        }
        .status.connecting {
            background-color: #ffd;
            color: #cc3;
        }
        button {
            background-color: #4CAF50;
            color: white;
            padding: 12px 24px;
            border: none;
            border-radius: 5px;
            cursor: pointer;
            font-size: 16px;
            margin: 10px 5px;
            width: 200px;
        }
        button:hover {
            background-color: #45a049;
        }
        button:disabled {
            background-color: #cccccc;
            cursor: not-allowed;
        }
        button.stop {
            background-color: #f44336;
        }
        button.stop:hover {
            background-color: #da190b;
        }
        .controls {
            text-align: center;
            margin: 30px 0;
        }
        .info {
            background-color: #e3f2fd;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            border-left: 4px solid #2196F3;
        }
        .log {
            background-color: #f9f9f9;
            padding: 15px;
            border-radius: 5px;
            max-height: 200px;
            overflow-y: auto;
            font-family: monospace;
            font-size: 12px;
            margin-top: 20px;
        }
        .log-entry {
            margin: 5px 0;
            padding: 5px;
            border-left: 3px solid #ddd;
        }
        .log-entry.error {
            border-left-color: #f44336;
            color: #c33;
        }
        .log-entry.info {
            border-left-color: #2196F3;
            color: #333;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>üéôÔ∏è Pipecat Speech2Speech Test Client</h1>
        
        <div id="status" class="status disconnected">Disconnected</div>
        
        <div class="info">
            <strong>Instructions:</strong>
            <ol>
                <li>Click "Connect" to establish WebSocket connection</li>
                <li>Allow microphone access when prompted</li>
                <li>Start speaking - your speech will be transcribed and processed</li>
                <li>You'll hear the AI assistant's response</li>
                <li>Click "Disconnect" when done</li>
            </ol>
        </div>
        
        <div class="controls">
            <button id="connectBtn" onclick="connect()">Connect</button>
            <button id="disconnectBtn" onclick="disconnect()" class="stop" disabled>Disconnect</button>
        </div>
        
        <div class="log" id="log">
            <div class="log-entry info">Ready to connect...</div>
        </div>
    </div>

    <script>
        let ws = null;
        let mediaStream = null;
        let audioContext = null;
        let sourceNode = null;
        let destinationNode = null;
        let processorNode = null;
        const WS_URL = 'ws://localhost:8000/ws';
        const SAMPLE_RATE = 16000;
        const CHANNELS = 1;

        function log(message, type = 'info') {
            const logDiv = document.getElementById('log');
            const entry = document.createElement('div');
            entry.className = `log-entry ${type}`;
            entry.textContent = `[${new Date().toLocaleTimeString()}] ${message}`;
            logDiv.appendChild(entry);
            logDiv.scrollTop = logDiv.scrollHeight;
        }

        function updateStatus(status, className) {
            const statusDiv = document.getElementById('status');
            statusDiv.textContent = status;
            statusDiv.className = `status ${className}`;
        }

        async function connect() {
            try {
                updateStatus('Connecting...', 'connecting');
                log('Connecting to WebSocket...', 'info');
                
                // Connect WebSocket
                ws = new WebSocket(WS_URL);
                
                ws.onopen = () => {
                    log('WebSocket connected!', 'info');
                    updateStatus('Connected', 'connected');
                    document.getElementById('connectBtn').disabled = true;
                    document.getElementById('disconnectBtn').disabled = false;
                    startAudio();
                };
                
                ws.onmessage = async (event) => {
                    if (event.data instanceof Blob) {
                        // Audio data received (protobuf)
                        await handleAudioData(event.data);
                    } else if (event.data instanceof ArrayBuffer) {
                        // Audio data received (protobuf)
                        await handleAudioData(event.data);
                    } else {
                        log(`Message received: ${event.data}`, 'info');
                    }
                };
                
                ws.onerror = (error) => {
                    log(`WebSocket error: ${error}`, 'error');
                    updateStatus('Error', 'disconnected');
                };
                
                ws.onclose = () => {
                    log('WebSocket disconnected', 'info');
                    updateStatus('Disconnected', 'disconnected');
                    document.getElementById('connectBtn').disabled = false;
                    document.getElementById('disconnectBtn').disabled = true;
                    stopAudio();
                };
                
            } catch (error) {
                log(`Connection error: ${error.message}`, 'error');
                updateStatus('Error', 'disconnected');
            }
        }

        async function startAudio() {
            try {
                log('Requesting microphone access...', 'info');
                
                // Get user media
                mediaStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: SAMPLE_RATE,
                        channelCount: CHANNELS,
                        echoCancellation: true,
                        noiseSuppression: true
                    }
                });
                
                log('Microphone access granted', 'info');
                
                // Create audio context
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: SAMPLE_RATE
                });
                
                // Create source from microphone
                sourceNode = audioContext.createMediaStreamSource(mediaStream);
                
                // Create destination for playback
                destinationNode = audioContext.createMediaStreamDestination();
                
                // Create script processor for sending audio
                processorNode = audioContext.createScriptProcessor(4096, CHANNELS, CHANNELS);
                
                processorNode.onaudioprocess = (event) => {
                    if (ws && ws.readyState === WebSocket.OPEN) {
                        const inputData = event.inputBuffer.getChannelData(0);
                        // Convert Float32Array to Int16Array
                        const int16Data = new Int16Array(inputData.length);
                        for (let i = 0; i < inputData.length; i++) {
                            const s = Math.max(-1, Math.min(1, inputData[i]));
                            int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
                        }
                        // Serialize as protobuf InputAudioRawFrame and send
                        const protobufFrame = serializeAudioFrame(int16Data.buffer, SAMPLE_RATE, CHANNELS);
                        if (protobufFrame) {
                            ws.send(protobufFrame);
                        }
                    }
                };
                
                sourceNode.connect(processorNode);
                processorNode.connect(audioContext.destination);
                
                log('Audio streaming started', 'info');
                
            } catch (error) {
                log(`Audio error: ${error.message}`, 'error');
                log('Make sure you allow microphone access', 'error');
            }
        }

        async function handleAudioData(data) {
            try {
                let arrayBuffer;
                // Convert received data to ArrayBuffer
                if (data instanceof Blob) {
                    arrayBuffer = await data.arrayBuffer();
                } else if (data instanceof ArrayBuffer) {
                    arrayBuffer = data;
                } else {
                    log(`Unknown data type: ${typeof data}`, 'error');
                    return;
                }
                
                // Try to deserialize as protobuf frame
                try {
                    // Load protobuf schema (simplified - we'll parse manually for now)
                    const audioData = deserializeProtobufAudio(arrayBuffer);
                    if (audioData) {
                        playAudio(audioData);
                    } else {
                        // If deserialization fails, try playing as raw audio
                        playAudio(arrayBuffer);
                    }
                } catch (error) {
                    log(`Protobuf deserialization error: ${error.message}`, 'error');
                    // Fallback: try playing as raw audio
                    playAudio(arrayBuffer);
                }
            } catch (error) {
                log(`Error handling audio: ${error.message}`, 'error');
            }
        }
        
        function serializeAudioFrame(audioBuffer, sampleRate, numChannels) {
            try {
                // Protobuf wire format for InputAudioRawFrame
                // Frame message: oneof frame { AudioRawFrame audio = 2; }
                // AudioRawFrame: bytes audio = 3; uint32 sample_rate = 4; uint32 num_channels = 5;
                
                const audioBytes = new Uint8Array(audioBuffer);
                const audioLength = audioBytes.length;
                
                // Build protobuf message manually
                const parts = [];
                
                // Frame.audio field (field 2, wire type 2 = length-delimited)
                parts.push(0x12); // Field 2, wire type 2
                
                // Calculate AudioRawFrame message size
                let frameSize = 0;
                
                // audio field (field 3, wire type 2)
                frameSize += 1; // field tag
                frameSize += encodeVarint(audioLength).length; // length
                frameSize += audioLength; // data
                
                // sample_rate field (field 4, wire type 0 = varint)
                frameSize += 1; // field tag
                frameSize += encodeVarint(sampleRate).length; // value
                
                // num_channels field (field 5, wire type 0 = varint)
                frameSize += 1; // field tag
                frameSize += encodeVarint(numChannels).length; // value
                
                // Write frame length
                parts.push(...encodeVarint(frameSize));
                
                // Write audio field (field 3)
                parts.push(0x1A); // Field 3, wire type 2
                parts.push(...encodeVarint(audioLength));
                parts.push(...audioBytes);
                
                // Write sample_rate field (field 4)
                parts.push(0x20); // Field 4, wire type 0
                parts.push(...encodeVarint(sampleRate));
                
                // Write num_channels field (field 5)
                parts.push(0x28); // Field 5, wire type 0
                parts.push(...encodeVarint(numChannels));
                
                return new Uint8Array(parts).buffer;
            } catch (error) {
                log(`Serialization error: ${error.message}`, 'error');
                return null;
            }
        }
        
        function encodeVarint(value) {
            const bytes = [];
            while (value > 0x7F) {
                bytes.push((value & 0x7F) | 0x80);
                value >>>= 7;
            }
            bytes.push(value & 0x7F);
            return bytes;
        }
        
        function deserializeProtobufAudio(buffer) {
            try {
                // Simple protobuf parsing for AudioRawFrame
                // Protobuf wire format: field_number << 3 | wire_type
                // Frame message: oneof frame { AudioRawFrame audio = 2; }
                // AudioRawFrame: bytes audio = 3;
                
                // This is a simplified parser - for production, use protobuf.js properly
                // For now, we'll try to extract audio bytes from the protobuf structure
                
                const view = new Uint8Array(buffer);
                let offset = 0;
                
                // Look for audio frame (field 2 in Frame message)
                // Field 2 = 0x12 (field 2, wire type 2 = length-delimited)
                while (offset < view.length) {
                    if (view[offset] === 0x12) { // Field 2 (audio frame)
                        offset++;
                        // Read length (varint)
                        let length = 0;
                        let shift = 0;
                        while (offset < view.length) {
                            const byte = view[offset++];
                            length |= (byte & 0x7F) << shift;
                            if ((byte & 0x80) === 0) break;
                            shift += 7;
                        }
                        
                        // Now we're inside AudioRawFrame, look for audio field (field 3)
                        const frameStart = offset;
                        while (offset < frameStart + length && offset < view.length) {
                            if (view[offset] === 0x1A) { // Field 3 (audio bytes)
                                offset++;
                                // Read audio length
                                let audioLength = 0;
                                shift = 0;
                                while (offset < view.length) {
                                    const byte = view[offset++];
                                    audioLength |= (byte & 0x7F) << shift;
                                    if ((byte & 0x80) === 0) break;
                                    shift += 7;
                                }
                                
                                // Extract audio bytes
                                if (offset + audioLength <= view.length) {
                                    return buffer.slice(offset, offset + audioLength);
                                }
                            }
                            offset++;
                        }
                    }
                    offset++;
                }
                
                return null;
            } catch (error) {
                log(`Protobuf parsing error: ${error.message}`, 'error');
                return null;
            }
        }

        function playAudio(arrayBuffer) {
            if (!audioContext) return;
            
            try {
                // Ensure the buffer length is a multiple of 2 (Int16 = 2 bytes)
                const byteLength = arrayBuffer.byteLength || arrayBuffer.length;
                const alignedLength = Math.floor(byteLength / 2) * 2;
                
                if (alignedLength === 0) {
                    log('Received empty audio buffer', 'info');
                    return;
                }
                
                // Create a view of the aligned data
                const alignedBuffer = arrayBuffer.slice(0, alignedLength);
                const int16Data = new Int16Array(alignedBuffer);
                const float32Data = new Float32Array(int16Data.length);
                
                for (let i = 0; i < int16Data.length; i++) {
                    float32Data[i] = int16Data[i] / 32768.0;
                }
                
                // Create buffer and play
                const buffer = audioContext.createBuffer(1, float32Data.length, SAMPLE_RATE);
                buffer.getChannelData(0).set(float32Data);
                
                const source = audioContext.createBufferSource();
                source.buffer = buffer;
                source.connect(audioContext.destination);
                source.start();
            } catch (error) {
                log(`Playback error: ${error.message}`, 'error');
            }
        }

        function stopAudio() {
            if (processorNode) {
                processorNode.disconnect();
                processorNode = null;
            }
            if (sourceNode) {
                sourceNode.disconnect();
                sourceNode = null;
            }
            if (destinationNode) {
                destinationNode.disconnect();
                destinationNode = null;
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(track => track.stop());
                mediaStream = null;
            }
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close();
                audioContext = null;
            }
        }

        function disconnect() {
            if (ws) {
                ws.close();
                ws = null;
            }
            stopAudio();
            log('Disconnected', 'info');
        }

        // Cleanup on page unload
        window.addEventListener('beforeunload', () => {
            disconnect();
        });
    </script>
</body>
</html>

