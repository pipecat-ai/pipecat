- Added context summarization feature to automatically compress conversation history when conversation length limits (by token or message count) are reached, enabling efficient long-running conversations.
  - Configure via `enable_context_summarization=True` in `LLMAssistantAggregatorParams`
  - Customize behavior with `LLMContextSummarizationConfig` (max tokens, thresholds, etc.)
  - Automatically preserves incomplete function call sequences during summarization
  - See new examples: `examples/foundational/54-context-summarization-openai.py` and `examples/foundational/54a-context-summarization-google.py`
